{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear com NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa tarefa você vai estender sua implementação da tarefa passada para considerar múltiplas variáveis. Você pode estender a versão vetorizada implementada neste notebook para regressão simples. \n",
    "- Rode o algoritmo nesses dados, onde as linhas representam as notas de alunos de computação de alunos da UFCG em algumas disciplinas do primeiro período. A última coluna é a variável alvo representando o CRA final depois de concluir o curso. As outras colunas são algumas disciplinas do primeiro período. O pressuposto aqui é que as notas em disciplinas no primeiro período ajudam a explicar o CRA final dos alunos de computação.__\n",
    "- Compare o valor dos coeficientes estimados pelo seu algoritmo com o valor dos coeficientes da regressão linear do scikit learn para testar se o seu algoritmo está funcionando corretamente.\n",
    "\n",
    "A entrega deve ser o link no seu github para o notebook Jupyter com código python e texto explicativo quando necessário. De preferência, crie um repositório na sua conta do github e envie o link do html do notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versão Não Vetorizada\n",
    "Função para calcular o MSE (Mean Squared Error):\n",
    "\n",
    "$MSE(\\hat{w}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i (x_i))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_mse(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para fazer uma atualização dos parâmetros no Gradiente Descendente:\n",
    "\n",
    "$w_0 = w_0 + 2\\alpha\\sum_{i=1}^N (y_i - (w_0+w_1x_i))$\n",
    "\n",
    "$w_1 = w_1 + 2\\alpha\\sum_{i=1}^N x_i(y_i - (w_0+w_1x_i))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += (y - ((m_current * x) + b_current))\n",
    "        m_gradient += x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current + (2 * learningRate * b_gradient)\n",
    "    new_m = m_current + (2 * learningRate * m_gradient)\n",
    "    return [new_b, new_m, b_gradient, m_gradient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$||\\mathbf{w}||_2 = \\sqrt{\\sum_{j=1}^D w_j^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_2(x):\n",
    "    c=0\n",
    "    for i in range(len(x)):\n",
    "        c += x[i]**2\n",
    "    return math.sqrt(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para iterar sobre o gradiente descendente até convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, epsilon):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    grad = np.array([np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (norm_2(grad)>=epsilon):\n",
    "        b, m, b_gradient, m_gradient = step_gradient(b, m, points, learning_rate)\n",
    "        grad = np.array([b_gradient,m_gradient])\n",
    "        if i % 1000 == 0:\n",
    "            #print(norm_2(grad))\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse(b,m,points)))\n",
    "        i+= 1\n",
    "    return [b, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.genfromtxt(\"income.csv\", delimiter=\",\")\n",
    "learning_rate = 0.0001\n",
    "initial_b = 0 # initial y-intercept guess\n",
    "initial_m = 0 # initial slope guess\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.5\n",
    "print(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_mse(initial_b, initial_m, points)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "[b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, erro = {2}\".format(b, m, compute_mse(b, m, points)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versão Vetorizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MSE(\\hat{w})=\\frac{1}{N}(y-\\hat{\\mathbf{w}}^T\\mathbf{x})^T(y-\\hat{\\mathbf{w}}^T\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w,X,Y):\n",
    "    #print(\"w {0}, X {1}, Y {2}\".format(w, X, Y))\n",
    "    res = Y - np.dot(X,w).sum(axis=1) # lidando com mais de um X: SOMA? todos os valores de X (linha) para cada Y\n",
    "    totalError = np.dot(res.T, res)\n",
    "    return totalError / float(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current, X, Y, learningRate):\n",
    "    res = Y - np.dot(X, w_current).sum(axis=1) # lidando com mais de um X: SOMA? todos os valores de X (linha) para cada Y\n",
    "    b_gradient = np.sum(res)\n",
    "    X = X[:, 1:6]\n",
    "    \n",
    "    m_gradients = [b_gradient]\n",
    "    new_w = [w_current[0] + (2 * learningRate * b_gradient)]\n",
    "    for var in range(0, len(w_current)-1): # para cada variável, deve-se calcular o gradiente e o novo parâmetro w\n",
    "        var_gradient = np.sum(np.multiply(res, X[:, var]))\n",
    "        m_gradients.append(var_gradient)\n",
    "        \n",
    "        var_w = w_current[var+1] + (2 * learningRate * var_gradient)\n",
    "        new_w.append(var_w)\n",
    "    \n",
    "    return [new_w, m_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X, Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf,np.inf])\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad) >= epsilon):\n",
    "        w, m_gradients = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        #grad = np.array([b_gradient,m_gradient])\n",
    "        #print(np.linalg.norm(grad))\n",
    "        if i % 1000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i, compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    points = np.genfromtxt(\"sample_treino.csv\", delimiter=\",\")\n",
    "    points = np.c_[np.ones(len(points)),points]\n",
    "    X = points[1:, 1:-1] # independent variables\n",
    "    X = np.c_[np.ones(len(X)), X] # convert array to matrix\n",
    "    Y = points[1:, 5] # dependent variable\n",
    "    init_w = np.zeros((6,1)) # alterando para 6 linhas e 1 coluna\n",
    "    learning_rate = 0.0001\n",
    "    #num_iterations = 10000\n",
    "    epsilon = 0.5\n",
    "    \n",
    "    print(\"Executando...\")\n",
    "    tic = time.time()\n",
    "    w = gradient_descent_runner_vectorized(init_w, X, Y, learning_rate, epsilon)\n",
    "    toc = time.time()\n",
    "    print(\"Gradiente descendente convergiu com erro = {0} e:\".format(compute_mse_vectorized(w,X,Y))\n",
    "    for var in range(0, len(w)):\n",
    "          print(\"w_{0} = {1}\".format(var, w[var])\n",
    "    \n",
    "    print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
